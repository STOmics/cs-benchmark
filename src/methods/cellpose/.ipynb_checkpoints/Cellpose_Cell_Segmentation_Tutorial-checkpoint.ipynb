{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://pica.zhimg.com/70/v2-a491a7e651c77c8f91b2789a343ba1ac_1440w.awebp?source=172ae18b&biz_tag=Post\" width=60% height=60% alt=\"\" />\n",
    "    <h4>\n",
    "      Cellpose: a generalist algorithm for cellular segmentation\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "# What Is ***cellpose?***\n",
    "Cellpose is an open-source software tool and Python library designed for the automatic segmentation of cells in biomedical images, particularly in microscopy images. It was developed to streamline the process of identifying and segmenting individual cells within complex biological samples, making it a valuable tool for a wide range of applications in the fields of cell biology, neuroscience, and medical research.\n",
    "\n",
    "**Journal:** Nature methods<br>\n",
    "**DOI:** https://www.nature.com/articles/s41592-020-01018-x<br>\n",
    "**Preprint Date:** December 14, 2020<br>\n",
    "**Github:** https://github.com/MouseLand/cellpose<br>\n",
    "**Tutorial:** https://cellpose.readthedocs.io/en/latest/<br>\n",
    "**Environment (mirror):** Cellpose(URL: (URL: https://cloud.stomics.tech/#/public/image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial**      \n",
    "This code is used to segment cells in images. It includes steps such as image preprocessing, cell segmentation, post-processing, and image merging, ultimately producing segmented binary images.[Click](https://cellpose.readthedocs.io/en/latest/gui.html#training-your-own-cellpose-modelhttps://cellpose.readthedocs.io/en/latest/gui.html#training-your-own-cellpose-model) here for more tutorials. In this tutorial, we demonstrate how to apply <strong>cellpose</strong> algorithm.\n",
    "\n",
    "# **Input and Output**  \n",
    " \n",
    " >Input:the input of the original image is `*.tif`, the image type like 'FB' and 'mIf'...        \n",
    " >Output:the mask of cell segmentation will be saved in `*tif`, and the output value is binary, 0 or 255. \n",
    "\n",
    "# **Demo Data** \n",
    "\n",
    "|**Data**|**Size**|**Type**|**Platform**|**Donwload**|\n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "|data1| 512-508 | FB | Stereo-seq | [http://www.cellpose.org/dataset](http://www.cellpose.org/dataset) |\n",
    "|data2| 512-512 | mIf | Stereo-seq | [http://www.cellpose.org/dataset](http://www.cellpose.org/dataset) |\n",
    "     \n",
    "\n",
    "> Notes: this demo data are saved on cloud [click here](https://bgipan.genomics.cn/#/link/RMxRTwf7MrSPHwDsF2P9) (PWD:fgTG)\n",
    "\n",
    "\n",
    "# **Time Estimates** \n",
    "|**Data size**|**CPU Core**|**CPU Menory (G)**|**GPU Memory (G)**|**Running Time (min)**|\n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "|512-508|8|~1.7|~0|~5|\n",
    "\n",
    "\n",
    "<details close>\n",
    "<summary><b><font size=\"4\">Parameters</b></summary>\n",
    "\n",
    "- Input Image Path (open_path): Specifies the file path of the input image for cell segmentation.\n",
    "- Output Image Path (save_path): Specifies the file path to save the segmentation results.\n",
    "- Image Block Size (photo_size): Specifies the size of image blocks used for segmentation.\n",
    "- Image Block Step (photo_step): Specifies the step size between image blocks.\n",
    "- Minimum Cell Diameter (dmin): Specifies the minimum diameter of cells.\n",
    "- Maximum Cell Diameter (dmax): Specifies the maximum diameter of cells.\n",
    "- Step (step): Specifies the step size for cell diameters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Import Packages and Load Data**\n",
    "# **setup plugins**\n",
    "Download [Cellpose.zip](https://bgipan.genomics.cn/#/link/RMxRTwf7MrSPHwDsF2P9)(passwordï¼šfgTG) and and put it into the notebook pathway './work/'. then unpacked files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  Cellpose.zip\n",
      "  inflating: 330_img.tif             \n",
      "  inflating: 339_img.tif             \n",
      "   creating: cellpose/\n",
      "  inflating: cellpose/__init__.py    \n",
      "  inflating: cellpose/__main__.py    \n",
      "  inflating: cellpose/core.py        \n",
      "  inflating: cellpose/dynamics.py    \n",
      "  inflating: cellpose/io.py          \n",
      "  inflating: cellpose/metrics.py     \n",
      "  inflating: cellpose/models.py      \n",
      "   creating: cellpose/models/\n",
      "  inflating: cellpose/models/cellpose_residual_on_style_on_concatenation_off_train1_2022_10_28_16_59_59.600570  \n",
      "  inflating: cellpose/models/cellpose_residual_on_style_on_concatenation_off_train_2023_06_20_15_14_14.218607_epoch_9951  \n",
      "  inflating: cellpose/models/CP      \n",
      "  inflating: cellpose/models/CP_20230619_095147  \n",
      "  inflating: cellpose/models/CPx     \n",
      "  inflating: cellpose/models/cyto2torch_0  \n",
      "  inflating: cellpose/models/cyto2torch_1  \n",
      "  inflating: cellpose/models/cyto2torch_2  \n",
      "  inflating: cellpose/models/cyto2torch_3  \n",
      "  inflating: cellpose/models/cytotorch_0  \n",
      "  inflating: cellpose/models/cytotorch_1  \n",
      "  inflating: cellpose/models/cytotorch_2  \n",
      "  inflating: cellpose/models/cytotorch_3  \n",
      "  inflating: cellpose/models/demo    \n",
      "  inflating: cellpose/models/general  \n",
      "  inflating: cellpose/models/gui_models.txt  \n",
      "  inflating: cellpose/models/LC1     \n",
      "  inflating: cellpose/models/LC2     \n",
      "  inflating: cellpose/models/LC3     \n",
      "  inflating: cellpose/models/LC4     \n",
      "  inflating: cellpose/models/livecell  \n",
      "  inflating: cellpose/models/nucleitorch_0  \n",
      "  inflating: cellpose/models/size_cyto2torch_0.npy  \n",
      "  inflating: cellpose/models/size_cytotorch_0.npy  \n",
      "  inflating: cellpose/models/size_nucleitorch_0.npy  \n",
      "  inflating: cellpose/plot.py        \n",
      "  inflating: cellpose/resnet_torch.py  \n",
      "  inflating: cellpose/test_mkl.py    \n",
      "  inflating: cellpose/transforms.py  \n",
      "  inflating: cellpose/utils.py       \n"
     ]
    }
   ],
   "source": [
    "!unzip Cellpose.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import patchify\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import time, os, sys\n",
    "from math import ceil\n",
    "import patchify\n",
    "import cv2\n",
    "import tifffile as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "sys.path.append('.cellpose') \n",
    "sys.path.insert(0,'cellpose')\n",
    "from cellpose import io, models, utils\n",
    "\n",
    "class CellSegmentation:\n",
    "    def __init__(self, open_path, save_path, photo_size, photo_step, dmin, dmax, step):\n",
    "        self.open_path = open_path\n",
    "        self.save_path = save_path\n",
    "        self.photo_size = photo_size\n",
    "        self.photo_step = photo_step\n",
    "        self.dmin = dmin\n",
    "        self.dmax = dmax\n",
    "        self.step = step\n",
    "\n",
    "    def _process_image(self, img_data):\n",
    "        overlap = self.photo_size - self.photo_step\n",
    "        if (overlap % 2) == 1:\n",
    "            overlap = overlap + 1\n",
    "        act_step = ceil(overlap / 2)\n",
    "        im = cv2.imread(self.open_path)\n",
    "        dir_image1 = self.open_path.split('/')[-1].strip('.tif')\n",
    "        image = np.array(im)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        res_image = np.pad(gray_image, ((act_step, act_step), (act_step, act_step)), 'constant')\n",
    "        a = res_image.shape[0]\n",
    "        b = res_image.shape[1]\n",
    "        res_a = ceil((a-self.photo_size)/self.photo_step)*self.photo_step+self.photo_size\n",
    "        res_b = ceil((b-self.photo_size)/self.photo_step)*self.photo_step+self.photo_size\n",
    "        padding_rows = res_a - a\n",
    "        padding_cols = res_b - b\n",
    "        regray_image = np.pad(res_image, ((0, padding_rows), (0, padding_cols)), mode='constant')\n",
    "        patches = patchify.patchify(regray_image, (self.photo_size, self.photo_size), step=self.photo_step)     \n",
    "        wid = patches.shape[0]\n",
    "        high = patches.shape[1]\n",
    "        model = models.Cellpose(gpu=True, model_type='cyto')\n",
    "        a_patches = np.full((wid, high, (self.photo_step), (self.photo_step)), 255)\n",
    "\n",
    "        for i in range(wid):\n",
    "            for j in range(high):\n",
    "                img_data = patches[i, j, :, :]\n",
    "                num0min = wid * high * 800000000000000\n",
    "                for k in range(self.dmin, self.dmax, self.step):\n",
    "\n",
    "                    masks, flows, styles, diams = model.eval(img_data, diameter=k, channels=[0, 0], flow_threshold=0.9)\n",
    "                    num0 = np.sum(masks == 0)\n",
    "\n",
    "                    if num0 < num0min:\n",
    "                        num0min = num0\n",
    "                        outlines = utils.masks_to_outlines(masks)\n",
    "                        outlines = (outlines == True).astype(int) * 255\n",
    "\n",
    "                        try:\n",
    "                            a_patches[i, j, :, :] = outlines[act_step:(self.photo_step+act_step), act_step:(self.photo_step+act_step)]\n",
    "                            output = masks.copy()\n",
    "                        except:\n",
    "                            a_patches[i, j, :, :] = output[act_step:(self.photo_step+act_step), act_step:(self.photo_step+act_step)]\n",
    "\n",
    "        patch_nor = patchify.unpatchify(a_patches, ((wid) * (self.photo_step), (high) * (self.photo_step)))\n",
    "        nor_imgdata = np.array(patch_nor)\n",
    "        cropped_1 = nor_imgdata[0:gray_image.shape[0], 0:gray_image.shape[1]]\n",
    "        cropped_1 = np.uint8(cropped_1)\n",
    "        return cropped_1\n",
    "\n",
    "    def _post_image(self, process_image):\n",
    "        contour_thickness = 0\n",
    "        contour_coords = np.argwhere(process_image == 255)\n",
    "        distance_transform = distance_transform_edt(process_image == 0)\n",
    "        expanded_image = np.zeros_like(process_image)\n",
    "        for y, x in contour_coords:\n",
    "            mask = distance_transform[y, x] <= contour_thickness\n",
    "            expanded_image[y - contour_thickness:y + contour_thickness + 1, x - contour_thickness:x + contour_thickness + 1] = mask * 255\n",
    "        contours, _ = cv2.findContours(expanded_image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "        height, width = process_image.shape\n",
    "        black_background = np.zeros((height, width), dtype=np.uint8)\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if 20 < area < 10000:\n",
    "                cv2.drawContours(black_background, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        black_background = np.uint8(black_background)\n",
    "        return black_background, expanded_image\n",
    "\n",
    "    def _merger_image(self, merger_image1, merger_image2):\n",
    "        merger_image1[merger_image2==255]=0\n",
    "        return merger_image1\n",
    "\n",
    "    def segment_cells(self):\n",
    "        inverted_image = self._process_image(self.open_path)\n",
    "        post_image, expanded_image = self._post_image(inverted_image)\n",
    "        result_image = self._merger_image(post_image, expanded_image)\n",
    "        cv2.imwrite(self.save_path, result_image)\n",
    "        contours, _ = cv2.findContours(inverted_image, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)       \n",
    "        h, w = result_image.shape[:2]\n",
    "        outline = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        img = cv2.imread(self.open_path)\n",
    "        if img.ndim == 2:\n",
    "            show_r = cv2.cvtColor(img.copy(), cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            show_r = img.copy()\n",
    "        cv2.drawContours(show_r, contours=contours, contourIdx=-1, color=(255, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(show_r)\n",
    "        plt.title('Cell Segmentation, number of cells:{}'.format(len(contours)))\n",
    "        areas = [cv2.contourArea(c) for c in contours]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(areas, bins=len(areas) // 20, range=(0,len(areas)), color='skyblue', alpha=0.8)\n",
    "        plt.title('Cell area')\n",
    "        plt.xlabel('Value (pix)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run data**\n",
    "- Scene 1: Inference for FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src data type = object is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      9\u001b[0m cell_segmenter \u001b[38;5;241m=\u001b[39m CellSegmentation(open_path, save_path, photo_size, photo_step, dmin, dmax, step)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcell_segmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegment_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 101\u001b[0m, in \u001b[0;36mCellSegmentation.segment_cells\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msegment_cells\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     inverted_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     post_image, expanded_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_image(inverted_image)\n\u001b[1;32m    103\u001b[0m     result_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merger_image(post_image, expanded_image)\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mCellSegmentation._process_image\u001b[0;34m(self, img_data)\u001b[0m\n\u001b[1;32m     35\u001b[0m dir_image1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(im)\n\u001b[0;32m---> 37\u001b[0m gray_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m res_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(gray_image, ((act_step, act_step), (act_step, act_step)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m a \u001b[38;5;241m=\u001b[39m res_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src data type = object is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "open_path = \"/storeData/USER/data/01.CellBin/00.user/fanjinghong/code/benchmark/input/NuInsSeg/tissue_images/human_bladder_01.tif\"\n",
    "save_path = \"/storeData/USER/data/01.CellBin/00.user/fanjinghong/code/benchmark/output/NuInsSeg/339_img_mask.tif\"\n",
    "photo_size = 512\n",
    "photo_step = 512\n",
    "dmin = 30\n",
    "dmax = 40\n",
    "step = 5\n",
    "\n",
    "cell_segmenter = CellSegmentation(open_path, save_path, photo_size, photo_step, dmin, dmax, step)\n",
    "cell_segmenter.segment_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scene 2: Inference for mIf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_path = \"./330_img.tif\"\n",
    "save_path = \"./330_img_mask.tif\"\n",
    "photo_size = 512\n",
    "photo_step = 512\n",
    "dmin = 30\n",
    "dmax = 40\n",
    "step = 5\n",
    "\n",
    "cell_segmenter = CellSegmentation(open_path, save_path, photo_size, photo_step, dmin, dmax, step)\n",
    "cell_segmenter.segment_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Contact Information**\n",
    "For questions about this notebook, please contact: _cloud@stomics.tech_.\n",
    "\n",
    "# **Cite**\n",
    "If you use STOmics/Stereo-seq data in your research, please considering referring us in your article:\n",
    "> **Code available** The source code of this algorithm is available at Github (https://github.com/MouseLand/cellpose). The visual and convenient execution of this algorithm can be found from STOmics Cloud Platform (https://cloud.stomics.tech/).\n",
    "\n",
    "> **Acknowledgement** We express our gratitude to the computing platform STOmics Cloud (https://cloud.stomics.tech/) for enabling workflow automation and accelerating Stereo-seq data analysis. If you use STOmics/Stereo-seq data in your research, please considering referring us in your article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose",
   "language": "python",
   "name": "cellpose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4579901502b6ba19508e55ce74c8ff2cae3e8e693557b1b003d86d977fa0f9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
